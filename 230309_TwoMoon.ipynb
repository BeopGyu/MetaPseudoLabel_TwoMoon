{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = []\n",
    "device_names = []\n",
    "if torch.cuda.is_available():\n",
    "    for gpu_id in range(torch.cuda.device_count()):\n",
    "        gpu_ids += [gpu_id]\n",
    "        device_names += [torch.cuda.get_device_name(gpu_id)]\n",
    "print(gpu_ids)\n",
    "print(device_names)\n",
    "\n",
    "if len(gpu_ids) > 1:\n",
    "    device = 'cuda:' + str(gpu_ids[0])  # 여기서 gpu 번호 고르기\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "# num_samples = (1000,1000)\n",
    "\n",
    "points, labels = make_moons(n_samples=num_samples, shuffle=True ,noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_l = np.array((\n",
    "    (-0.9, 0.25),\n",
    "    (0.4, 1.05),\n",
    "    (0.9, 0.05),\n",
    "    # (0.2, 0.1), # class 2 왼쪽 끝\n",
    "    (0.7, -0.4),\n",
    "    (0.85, -0.35),\n",
    "    (1.9, 0.2)\n",
    "))\n",
    "labels_l = np.array((0,0,0,1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points[:,0],points[:,1], s=10)\n",
    "plt.scatter(points_l[:,0],points_l[:,1], s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = random.randint(-10, 10)\n",
    "while(offset == 0):\n",
    "    offset = random.randint(-10, 10)\n",
    "offset = 0\n",
    "\n",
    "points += offset\n",
    "points_l += offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points[:,0],points[:,1], s=10)\n",
    "plt.scatter(points_l[:,0],points_l[:,1], s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridshape = (3,4)\n",
    "# marker_size = 0.1\n",
    "# num = 12\n",
    "\n",
    "# jr = num//4\n",
    "# for i in range(4):\n",
    "#     for j in range(jr):\n",
    "#         loc = (j,i)\n",
    "#         ax = plt.subplot2grid(gridshape, loc)\n",
    "#         point_noise = points + (torch.rand(size=points.shape).numpy()-0.5) * (0.1*((i+1)+(jr+1)*j))\n",
    "#         plt.scatter(point_noise[:,0],point_noise[:,1], s=10)\n",
    "#         plt.scatter(points[:,0],points[:,1], s=10)\n",
    "#         plt.title(((i+1)+(jr+1)*j))\n",
    "\n",
    "# ax.figure.set(figwidth=28, figheight=6 * jr)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMClassifier(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=8),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(in_features=8, out_features=8),\n",
    "            # nn.Sigmoid(),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=8),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=2)\n",
    "        )\n",
    "\n",
    "        # self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.layers:\n",
    "            if(isinstance(layer, nn.Linear)):\n",
    "                nn.init.uniform_(layer.weight, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        self.x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "        self.y_data = torch.tensor(y_data, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.len = len(x_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)   #self.transform이 None이 아니라면 전처리\n",
    "        return sample \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = MyDataset(points_l, labels_l)\n",
    "unlabeledData = MyDataset(points, labels)\n",
    "\n",
    "labeledLoader = DataLoader(labeledData, batch_size=6, shuffle=True)\n",
    "unlabeledLoader = DataLoader(unlabeledData, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.5+offset, 2.5+offset, num=800)\n",
    "y = np.linspace(-1.0+offset, 1.5+offset, num=500)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "marker_size = 0.1\n",
    "# plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.stack((X.flatten(),Y.flatten()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderData = MyDataset(arr, np.zeros_like(arr))\n",
    "\n",
    "renderLoader = DataLoader(renderData, batch_size=1024, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_pseudo_labels_step(teacher_model,\n",
    "                            teacher_optimizer,\n",
    "                            student_model,\n",
    "                            student_optimizer,\n",
    "                            uo_data,\n",
    "                            ua_data,\n",
    "                            l_data,\n",
    "                            l_label):\n",
    "    teacher_optimizer.zero_grad()\n",
    "    student_optimizer.zero_grad()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    threshold = 0.6\n",
    "    lambda_u = 8\n",
    "\n",
    "    #### teacher UDA loss\n",
    "    output_t_uo = teacher_model(uo_data)\n",
    "    soft_pseudo_label = nn.functional.softmax(output_t_uo / 0.75, dim=-1).detach()\n",
    "    \n",
    "    output_t_ua = teacher_model(ua_data)\n",
    "    loss_t_u = torch.mul(soft_pseudo_label, torch.log_softmax(output_t_ua, dim=-1))\n",
    "    max_probs, _ = torch.max(soft_pseudo_label, dim=-1, keepdim=True)\n",
    "    mask = torch.greater_equal(max_probs, threshold).type(torch.float32)\n",
    "    \n",
    "    loss_t_u = torch.mean(torch.mul(-loss_t_u, mask)) # consistency loss\n",
    "\n",
    "    #### teacher supervised loss\n",
    "    output_t_l = teacher_model(l_data)\n",
    "    loss_t_supervised = loss_fn(nn.functional.softmax(output_t_l, dim=-1), l_label)\n",
    "\n",
    "    #### student performance old\n",
    "    output_l_old = student_model(l_data)\n",
    "    loss_l_old = loss_fn(nn.functional.softmax(output_l_old, dim=-1),\n",
    "                            l_label).detach()\n",
    "    \n",
    "    #### student loss\n",
    "    output_ua = student_model(ua_data)\n",
    "    student_loss = loss_fn(nn.functional.softmax(output_ua, dim=-1),\n",
    "                   torch.argmax(soft_pseudo_label, dim=-1).long())\n",
    "    \n",
    "    #### student update\n",
    "    student_optimizer.zero_grad()\n",
    "    student_loss.backward()\n",
    "    student_optimizer.step()\n",
    "\n",
    "    #### student performance new\n",
    "    output_l_new =student_model(l_data)\n",
    "    loss_l_new = loss_fn(nn.functional.softmax(output_l_new, dim=-1),\n",
    "                            l_label).detach()\n",
    "    \n",
    "    #### compute MPL loss\n",
    "    dot_product = (loss_l_new - loss_l_old)\n",
    "    loss_t_mpl = torch.argmax(nn.functional.softmax(output_t_ua, dim=-1), dim=-1, keepdim=True)\n",
    "    loss_t_mpl = -torch.mean(torch.mul(loss_t_mpl, nn.functional.log_softmax(output_t_ua, dim=-1)))\n",
    "    loss_t_mpl = loss_t_mpl * dot_product\n",
    "\n",
    "    #### teacher update\n",
    "    teacher_optimizer.zero_grad()\n",
    "    teacher_loss = loss_t_supervised + loss_t_u + loss_t_mpl\n",
    "    teacher_loss.backward()\n",
    "    teacher_optimizer.step()\n",
    "\n",
    "    return teacher_loss.detach().numpy(), student_loss.detach().numpy(), loss_t_mpl.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_iterator = unlabeledLoader.__iter__()\n",
    "labeled_iterator = labeledLoader.__iter__()\n",
    "\n",
    "supervised_mpl = TMClassifier()\n",
    "metaPseudoLabel = TMClassifier()\n",
    "\n",
    "supervised_mpl.train(True)\n",
    "metaPseudoLabel.train(True)\n",
    "\n",
    "lr = 2e-1\n",
    "opt_st = torch.optim.Adam(supervised_mpl.parameters(), lr=lr)\n",
    "opt_mpl = torch.optim.Adam(metaPseudoLabel.parameters(), lr=lr)\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 2e5+1\n",
    "\n",
    "best_acc = 0.8\n",
    "\n",
    "noise = 0.2\n",
    "\n",
    "while(True):\n",
    "    try:\n",
    "        uo_point, _ = unlabeled_iterator.__next__()\n",
    "        ua_point = uo_point + torch.rand(u_point.shape)*noise\n",
    "    except:\n",
    "        unlabeled_iterator = unlabeledLoader.__iter__()\n",
    "        u_point, _ = unlabeled_iterator.__next__()\n",
    "        ua_point = uo_point + torch.rand(u_point.shape)*noise\n",
    "    try:\n",
    "        l_point, l_label = labeled_iterator.__next__()\n",
    "    except:\n",
    "        labeled_iterator = labeledLoader.__iter__()\n",
    "        l_point, l_label = labeled_iterator.__next__()\n",
    "\n",
    "    \n",
    "    supervised_mpl.train(True)\n",
    "    metaPseudoLabel.train(True)\n",
    "    \n",
    "    step = meta_pseudo_labels_step(teacher_model=supervised_mpl,\n",
    "                                   teacher_optimizer=opt_st,\n",
    "                                   student_model=metaPseudoLabel,\n",
    "                                   student_optimizer=opt_mpl,\n",
    "                                   uo_data=uo_point,\n",
    "                                   ua_data=ua_point,\n",
    "                                   l_data=l_point,\n",
    "                                   l_label=l_label)\n",
    "    \n",
    "    supervised_mpl.train(False)\n",
    "    metaPseudoLabel.train(False)\n",
    "\n",
    "    infer = supervised_mpl(torch.tensor(points).type(torch.float32))\n",
    "    teacher_acc = sum((torch.argmax(infer, -1).numpy() == labels).astype(int)) / num_samples\n",
    "    \n",
    "    infer = metaPseudoLabel(torch.tensor(points).type(torch.float32))\n",
    "    student_acc = sum((torch.argmax(infer, -1).numpy() == labels).astype(int)) / num_samples\n",
    "\n",
    "    if(student_acc > best_acc):\n",
    "            best_acc = student_acc\n",
    "            best_student_model = copy.deepcopy(metaPseudoLabel)\n",
    "\n",
    "    if float(student_acc) > 0.99:\n",
    "        break\n",
    "    \n",
    "    if(epoch%1e3 == 0):\n",
    "        print(f'Epoch: {epoch} \\tTeacher Loss: {step[0].item():.4f} \\tStudent Loss: {step[1].item():.4f} \\tMPL Loss: {step[2].item():.4f}')\n",
    "        print(f'\\tTeacher Acc: {teacher_acc.item()*num_samples} / {num_samples} \\tStudent Acc: {student_acc.item()*num_samples} / {num_samples}')\n",
    "\n",
    "\n",
    "    if(epoch%5e3 == 0):\n",
    "        try:\n",
    "            s_mpl_preds = np.array([])\n",
    "            mpl_preds = np.array([])\n",
    "\n",
    "            for point, _ in renderLoader:\n",
    "                _, s_mpl_pred = torch.max(supervised_mpl(point.float().to(device)), 1)\n",
    "                _, mpl_pred = torch.max(metaPseudoLabel(point.float().to(device)), dim=-1)\n",
    "\n",
    "                s_mpl_preds = np.concatenate((s_mpl_preds,s_mpl_pred.cpu().numpy()))\n",
    "                mpl_preds = np.concatenate((mpl_preds,mpl_pred.cpu().numpy()))\n",
    "\n",
    "            s_mpl_pred_points_0 = []\n",
    "            s_mpl_pred_points_1 = []\n",
    "            mpl_pred_points_0 = []\n",
    "            mpl_pred_points_1 = []\n",
    "\n",
    "            for i in range(arr.shape[0]):\n",
    "                if(s_mpl_preds[i] == 0):\n",
    "                    s_mpl_pred_points_0.append(arr[i])\n",
    "                else:\n",
    "                    s_mpl_pred_points_1.append(arr[i])\n",
    "                if(mpl_preds[i] == 0):\n",
    "                    mpl_pred_points_0.append(arr[i])\n",
    "                else:\n",
    "                    mpl_pred_points_1.append(arr[i])\n",
    "\n",
    "                    gridshape = (2, 2)\n",
    "            \n",
    "            gridshape = (1,2)\n",
    "\n",
    "            loc = (0,0)\n",
    "            ax = plt.subplot2grid(gridshape, loc)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            plt.scatter(np.array(s_mpl_pred_points_0)[:,0], np.array(s_mpl_pred_points_0)[:,1], s=marker_size)\n",
    "            plt.scatter(np.array(s_mpl_pred_points_1)[:,0], np.array(s_mpl_pred_points_1)[:,1], s=marker_size)\n",
    "            plt.scatter(points[:,0],points[:,1])\n",
    "            plt.scatter(points_l[:,0],points_l[:,1])\n",
    "            plt.title('Supervised - MPL')\n",
    "\n",
    "            loc = (0,1)\n",
    "            ax = plt.subplot2grid(gridshape, loc)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            plt.scatter(np.array(mpl_pred_points_0)[:,0], np.array(mpl_pred_points_0)[:,1], s=marker_size)\n",
    "            plt.scatter(np.array(mpl_pred_points_1)[:,0], np.array(mpl_pred_points_1)[:,1], s=marker_size)\n",
    "            plt.scatter(points[:,0],points[:,1])\n",
    "            plt.scatter(points_l[:,0],points_l[:,1])\n",
    "            plt.title('Meta Pesudo Label')\n",
    "\n",
    "            ax.figure.set(figwidth=14, figheight=6)\n",
    "\n",
    "            plt.show()\n",
    "        except:\n",
    "            print('pass')\n",
    "\n",
    "    epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(False):\n",
    "# while(epoch < max_epoch):\n",
    "    for point, label in unlabeledLoader:\n",
    "        ##### concatenate unlabeled data and labeled data for efficiency\n",
    "        batch_size = point.shape[0]\n",
    "        point_noise = point + torch.rand(size=point.shape) * noise\n",
    "        # point_noise = point + (torch.rand(size=point.shape).numpy()-0.5) * noise\n",
    "        points_ = torch.concat((point.to(device), point_noise.to(device), points_l_tensor)).float()\n",
    "        \n",
    "        # opt_mpl.zero_grad()\n",
    "        # opt_st.zero_grad()\n",
    "        \n",
    "        ##### run teacher model\n",
    "        # output_t = supervised_mpl(points_)\n",
    "\n",
    "        # output_t_uo = output_t[:batch_size] # unlabeled original\n",
    "        # output_t_ua = output_t[batch_size:batch_size*2] # unlabeled augmented\n",
    "        # output_t_l = output_t[batch_size*2:] # labeled\n",
    "        # del output_t\n",
    "        output_t_uo = supervised_mpl(point.to(device))\n",
    "        \n",
    "        # loss_t_supervised = creterion(torch.softmax(output_t_l, dim=-1), labels_l_tensor)\n",
    "\n",
    "        ##### get pseudo_label & compute uda loss\n",
    "        # # soft_pseudo_label = torch.softmax(output_t_uo, dim=-1)\n",
    "        # # soft_pseudo_label = torch.softmax(output_t_uo.detach(), dim=-1)\n",
    "        soft_pseudo_label = torch.softmax(output_t_uo / 0.7, dim=-1).detach() # args.temperture\n",
    "\n",
    "        max_probs, _ = torch.max(soft_pseudo_label, dim=-1, keepdim=True)\n",
    "        mask = torch.greater_equal(max_probs, threshold).type(torch.float32)\n",
    "        # mask = max_probs.ge(threshold).float()  # greater_equal\n",
    "        # weight_u = lambda_u * min(1., (epoch + 1) / 500.)\n",
    "        # # loss_t_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(output_t_ua, dim=-1)).sum(dim=-1)) # consistency loss\n",
    "        loss_t_u = torch.mul(soft_pseudo_label, torch.log_softmax(output_t_ua, dim=-1))\n",
    "        loss_t_u = torch.mean(torch.mul(-loss_t_u, mask)) # consistency loss\n",
    "        loss_t_uda = loss_t_u\n",
    "        # # loss_t_uda = loss_t_u * lambda_u\n",
    "        # loss_t_uda = loss_t_u * weight_u\n",
    "        \n",
    "        # soft_pseudo_label = torch.softmax(output_t_uo.detach() / 0.7, dim=-1)\n",
    "        # loss_t_uda = creterion(output_t_ua, soft_pseudo_label)\n",
    "\n",
    "        ##### run student model\n",
    "        # output = metaPseudoLabel(points_)\n",
    "        # output_ua = output[batch_size:batch_size*2]\n",
    "        # output_l = output[batch_size*2:]\n",
    "        # del output\n",
    "        output_ua = metaPseudoLabel(point_noise.to(device).float())\n",
    "        output_l = metaPseudoLabel(points_l_tensor.to(device))\n",
    "\n",
    "        loss_old_l = creterion(nn.functional.softmax(output_l, dim=-1),\n",
    "                               labels_l_tensor).detach()\n",
    "\n",
    "        # student is trained with augmented data \n",
    "        # https://github.com/google-research/google-research/issues/534#issuecomment-769559165\n",
    "\n",
    "        opt_mpl.zero_grad()\n",
    "\n",
    "        loss = creterion(nn.functional.softmax(output_ua, dim=-1),\n",
    "                        torch.argmax(soft_pseudo_label, dim=-1).long())\n",
    "        # loss = creterion(output_ua, torch.softmax(output_t_uo.detach(), dim=-1))    # get loss of student on unlabeled augmented input using pseudo label\n",
    "        # loss = creterion(output_ua, output_t_uo.detach())    # get loss of student on unlabeled augmented input using pseudo label\n",
    "\n",
    "        ##### update student\n",
    "        \n",
    "        loss.backward()#retain_graph=True)\n",
    "        opt_mpl.step()\n",
    "\n",
    "        ##### compute MPL loss with updated student\n",
    "        output_new_l = metaPseudoLabel(points_l_tensor)\n",
    "        loss_new_l = creterion(nn.functional.softmax(output_new_l, dim=-1),\n",
    "                               labels_l_tensor).detach()\n",
    "                               \n",
    "        dot_product = (loss_new_l - loss_old_l)\n",
    "        loss_t_mpl = torch.argmax(nn.functional.softmax(output_t_ua, dim=-1), dim=-1, keepdim=True)\n",
    "        loss_t_mpl = -torch.mean(torch.mul(loss_t_mpl, nn.functional.log_softmax(output_t_ua, dim=-1)))\n",
    "\n",
    "        # output_l = metaPseudoLabel(points_l_tensor)\n",
    "        # loss_t_mpl = creterion(output_l, labels_l_tensor)\n",
    "        # loss_t_mpl = creterion(output_l.detach(), labels_l_tensor)\n",
    "\n",
    "        opt_st.zero_grad()\n",
    "        loss_t = loss_t_mpl + loss_t_supervised + loss_t_uda\n",
    "        loss_t.backward()\n",
    "        opt_st.step()\n",
    "\n",
    "    \n",
    "    infer = metaPseudoLabel(point.to(device))\n",
    "    student_acc = sum((torch.argmax(infer, -1) == label.to(device)).type(torch.int))\n",
    "\n",
    "    if(student_acc/batch_size > best_acc):\n",
    "            best_acc = student_acc\n",
    "            best_student_model = copy.deepcopy(metaPseudoLabel)\n",
    "\n",
    "    if float(student_acc/batch_size) > 0.99:\n",
    "        break\n",
    "    \n",
    "    if(epoch%1e3 == 0):\n",
    "        _, preds = torch.max(output_t_uo, dim=-1)\n",
    "        acc_t = torch.sum(preds == label.to(device))\n",
    "        \n",
    "\n",
    "        print(f'Epoch: {epoch} \\tLoss: {loss.item():.4f} \\tTeacher Acc: {acc_t.item()} / {point.shape[0]} \\tMPL Acc: {student_acc.item()} / {point.shape[0]}')\n",
    "        print(f'\\t UDA loss: {loss_t_uda:.4f}\\t MPL loss: {loss_t_mpl:.4f}\\t Supervised loss: {loss_t_supervised:.4f}')\n",
    "\n",
    "\n",
    "    if(epoch%5e3 == 0):\n",
    "        try:\n",
    "            s_mpl_preds = np.array([])\n",
    "            mpl_preds = np.array([])\n",
    "\n",
    "            for point, _ in renderLoader:\n",
    "                _, s_mpl_pred = torch.max(supervised_mpl(point.float().to(device)), 1)\n",
    "                _, mpl_pred = torch.max(metaPseudoLabel(point.float().to(device)), dim=-1)\n",
    "\n",
    "                s_mpl_preds = np.concatenate((s_mpl_preds,s_mpl_pred.cpu().numpy()))\n",
    "                mpl_preds = np.concatenate((mpl_preds,mpl_pred.cpu().numpy()))\n",
    "\n",
    "            s_mpl_pred_points_0 = []\n",
    "            s_mpl_pred_points_1 = []\n",
    "            mpl_pred_points_0 = []\n",
    "            mpl_pred_points_1 = []\n",
    "\n",
    "            for i in range(arr.shape[0]):\n",
    "                if(s_mpl_preds[i] == 0):\n",
    "                    s_mpl_pred_points_0.append(arr[i])\n",
    "                else:\n",
    "                    s_mpl_pred_points_1.append(arr[i])\n",
    "                if(mpl_preds[i] == 0):\n",
    "                    mpl_pred_points_0.append(arr[i])\n",
    "                else:\n",
    "                    mpl_pred_points_1.append(arr[i])\n",
    "\n",
    "                    gridshape = (2, 2)\n",
    "            \n",
    "            gridshape = (1,2)\n",
    "\n",
    "            loc = (0,0)\n",
    "            ax = plt.subplot2grid(gridshape, loc)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            plt.scatter(np.array(s_mpl_pred_points_0)[:,0], np.array(s_mpl_pred_points_0)[:,1], s=marker_size)\n",
    "            plt.scatter(np.array(s_mpl_pred_points_1)[:,0], np.array(s_mpl_pred_points_1)[:,1], s=marker_size)\n",
    "            plt.scatter(points[:,0],points[:,1])\n",
    "            plt.scatter(points_l[:,0],points_l[:,1])\n",
    "            plt.title('Supervised - MPL')\n",
    "\n",
    "            loc = (0,1)\n",
    "            ax = plt.subplot2grid(gridshape, loc)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            plt.scatter(np.array(mpl_pred_points_0)[:,0], np.array(mpl_pred_points_0)[:,1], s=marker_size)\n",
    "            plt.scatter(np.array(mpl_pred_points_1)[:,0], np.array(mpl_pred_points_1)[:,1], s=marker_size)\n",
    "            plt.scatter(points[:,0],points[:,1])\n",
    "            plt.scatter(points_l[:,0],points_l[:,1])\n",
    "            plt.title('Meta Pesudo Label')\n",
    "\n",
    "            ax.figure.set(figwidth=14, figheight=6)\n",
    "\n",
    "            plt.show()\n",
    "        except:\n",
    "            print('pass')\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDEA_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
